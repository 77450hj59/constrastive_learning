{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/huaj/miniconda3/envs/gra/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %load data.py\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "import os.path as osp\n",
    "from torch_geometric.io import read_npz\n",
    "from typing import Optional, Callable\n",
    "\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, name, \n",
    "                transform: Optional[Callable] = None,\n",
    "                pre_transform: Optional[Callable] = None):\n",
    "        self.name = name.lower()\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'processed')    \n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return f'{self.name}.npz'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        # data_list = read_npz(self.raw_paths[0])\n",
    "        # make data\n",
    "        edge_index = torch.tensor([[1, 1],\n",
    "                                    [0, 2]],dtype=torch.long)\n",
    "\n",
    "        #edge_type\n",
    "        edge_type = torch.tensor([1,1])\n",
    "\n",
    "        # feature\n",
    "        X = torch.tensor([[0, 1, 2],\n",
    "                            [1, 0, 0],\n",
    "                            [0, 1, 1]], dtype=torch.float)\n",
    "        # X = torch.randn(3,3)\n",
    "\n",
    "        # lable\n",
    "        Y = torch.tensor([[0,0],[1,1],[2,0]])\n",
    "        data = Data(x=X, edge_index=edge_index, edge_type=edge_type, y=Y)\n",
    "\n",
    "        # make datalist\n",
    "        data_list = [data]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        # print(slices)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.name.capitalize()}Full()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self, ft_in, nb_classes):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.fc = nn.Linear(ft_in, nb_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        ret = self.fc(seq)\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, activation,\n",
    "                 base_model=GCNConv, k: int = 2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        assert k >= 2\n",
    "        self.k = k\n",
    "        self.conv = [base_model(in_channels, 2 * out_channels)]\n",
    "        for _ in range(1, k-1):\n",
    "            self.conv.append(base_model(2 * out_channels, 2 * out_channels))\n",
    "        self.conv.append(base_model(2 * out_channels, out_channels))\n",
    "        self.conv = nn.ModuleList(self.conv)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):\n",
    "        for i in range(self.k):\n",
    "            x = self.activation(self.conv[i](x, edge_index))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, encoder: Encoder, num_hidden: int, num_proj_hidden: int,\n",
    "                 tau: float = 0.5):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder: Encoder = encoder\n",
    "        self.tau: float = tau\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(num_hidden, num_proj_hidden)\n",
    "        self.fc2 = torch.nn.Linear(num_proj_hidden, num_hidden)\n",
    "\n",
    "    def forward(self, x: torch.Tensor,\n",
    "                edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        return self.encoder(x, edge_index)\n",
    "\n",
    "    def projection(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        z = F.elu(self.fc1(z))\n",
    "        return self.fc2(z)\n",
    "\n",
    "    def sim(self, z1: torch.Tensor, z2: torch.Tensor):\n",
    "        z1 = F.normalize(z1)\n",
    "        z2 = F.normalize(z2)\n",
    "        return torch.mm(z1, z2.t())\n",
    "\n",
    "    def semi_loss(self, z1: torch.Tensor, z2: torch.Tensor):\n",
    "        f = lambda x: torch.exp(x / self.tau)\n",
    "        refl_sim = f(self.sim(z1, z1))\n",
    "        between_sim = f(self.sim(z1, z2))\n",
    "\n",
    "        return -torch.log(\n",
    "            between_sim.diag()\n",
    "            / (refl_sim.sum(1) + between_sim.sum(1) - refl_sim.diag()))\n",
    "\n",
    "    def batched_semi_loss(self, z1: torch.Tensor, z2: torch.Tensor,\n",
    "                          batch_size: int):\n",
    "        # Space complexity: O(BN) (semi_loss: O(N^2))\n",
    "        device = z1.device\n",
    "        num_nodes = z1.size(0)\n",
    "        num_batches = (num_nodes - 1) // batch_size + 1\n",
    "        f = lambda x: torch.exp(x / self.tau)\n",
    "        indices = torch.arange(0, num_nodes).to(device)\n",
    "        losses = []\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            mask = indices[i * batch_size:(i + 1) * batch_size]\n",
    "            refl_sim = f(self.sim(z1[mask], z1))  # [B, N]\n",
    "            between_sim = f(self.sim(z1[mask], z2))  # [B, N]\n",
    "\n",
    "            losses.append(-torch.log(\n",
    "                between_sim[:, i * batch_size:(i + 1) * batch_size].diag()\n",
    "                / (refl_sim.sum(1) + between_sim.sum(1)\n",
    "                   - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())))\n",
    "\n",
    "        return torch.cat(losses)\n",
    "\n",
    "    def loss(self, z1: torch.Tensor, z2: torch.Tensor,\n",
    "             mean: bool = True, batch_size: int = 0):\n",
    "        h1 = self.projection(z1)\n",
    "        h2 = self.projection(z2)\n",
    "\n",
    "        if batch_size == 0:\n",
    "            l1 = self.semi_loss(h1, h2)\n",
    "            l2 = self.semi_loss(h2, h1)\n",
    "        else:\n",
    "            l1 = self.batched_semi_loss(h1, h2, batch_size)\n",
    "            l2 = self.batched_semi_loss(h2, h1, batch_size)\n",
    "\n",
    "        ret = (l1 + l2) * 0.5\n",
    "        ret = ret.mean() if mean else ret.sum()\n",
    "\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_feature(x, drop_prob):\n",
    "    drop_mask = torch.empty(\n",
    "        (x.size(1), ),\n",
    "        dtype=torch.float32,\n",
    "        device=x.device).uniform_(0, 1) < drop_prob\n",
    "    x = x.clone()\n",
    "    x[:, drop_mask] = 0\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load eval.py\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "\n",
    "\n",
    "def repeat(n_times):\n",
    "    def decorator(f):\n",
    "        @functools.wraps(f)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            results = [f(*args, **kwargs) for _ in range(n_times)]\n",
    "            statistics = {}\n",
    "            for key in results[0].keys():\n",
    "                values = [r[key] for r in results]\n",
    "                statistics[key] = {\n",
    "                    'mean': np.mean(values),\n",
    "                    'std': np.std(values)}\n",
    "            print_statistics(statistics, f.__name__)\n",
    "            return statistics\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def prob_to_one_hot(y_pred):\n",
    "    ret = np.zeros(y_pred.shape, np.bool)\n",
    "    indices = np.argmax(y_pred, axis=1)\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        ret[i][indices[i]] = True\n",
    "    return ret\n",
    "\n",
    "\n",
    "def print_statistics(statistics, function_name):\n",
    "    print(f'(E) | {function_name}:', end=' ')\n",
    "    for i, key in enumerate(statistics.keys()):\n",
    "        mean = statistics[key]['mean']\n",
    "        std = statistics[key]['std']\n",
    "        print(f'{key}={mean:.4f}+-{std:.4f}', end='')\n",
    "        if i != len(statistics.keys()) - 1:\n",
    "            print(',', end=' ')\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "\n",
    "@repeat(3)\n",
    "def label_classification(embeddings, y, ratio):\n",
    "    X = embeddings.detach().cpu().numpy()\n",
    "    Y = y.detach().cpu().numpy()\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    onehot_encoder = OneHotEncoder(categories='auto').fit(Y)\n",
    "    Y = onehot_encoder.transform(Y).toarray().astype(np.bool)\n",
    "\n",
    "    X = normalize(X, norm='l2')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                        test_size=1 - ratio)\n",
    "\n",
    "    logreg = LogisticRegression(solver='liblinear')\n",
    "    c = 2.0 ** np.arange(-10, 10)\n",
    "\n",
    "    clf = GridSearchCV(estimator=OneVsRestClassifier(logreg),\n",
    "                       param_grid=dict(estimator__C=c), n_jobs=8, cv=5,\n",
    "                       verbose=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    y_pred = prob_to_one_hot(y_pred)\n",
    "\n",
    "    micro = f1_score(y_test, y_pred, average=\"micro\")\n",
    "    macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        'F1Mi': micro,\n",
    "        'F1Ma': macro\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(T) | Epoch=001, loss=1.6095, this epoch 1.3790, total 1.3790\n",
      "(T) | Epoch=002, loss=1.6095, this epoch 0.0138, total 1.3927\n",
      "(T) | Epoch=003, loss=1.6094, this epoch 0.0114, total 1.4041\n",
      "(T) | Epoch=004, loss=1.6093, this epoch 0.0112, total 1.4154\n",
      "(T) | Epoch=005, loss=1.6093, this epoch 0.0114, total 1.4267\n",
      "(T) | Epoch=006, loss=1.6094, this epoch 0.0113, total 1.4380\n",
      "(T) | Epoch=007, loss=1.6126, this epoch 0.0112, total 1.4492\n",
      "(T) | Epoch=008, loss=1.6091, this epoch 0.0113, total 1.4605\n",
      "(T) | Epoch=009, loss=1.6092, this epoch 0.0111, total 1.4716\n",
      "(T) | Epoch=010, loss=1.6087, this epoch 0.0144, total 1.4860\n",
      "(T) | Epoch=011, loss=1.6093, this epoch 0.0111, total 1.4971\n",
      "(T) | Epoch=012, loss=1.6094, this epoch 0.0107, total 1.5078\n",
      "(T) | Epoch=013, loss=1.6092, this epoch 0.0106, total 1.5184\n",
      "(T) | Epoch=014, loss=1.6087, this epoch 0.0104, total 1.5288\n",
      "(T) | Epoch=015, loss=1.6093, this epoch 0.0103, total 1.5391\n",
      "(T) | Epoch=016, loss=1.6090, this epoch 0.0104, total 1.5496\n",
      "(T) | Epoch=017, loss=1.6114, this epoch 0.0106, total 1.5602\n",
      "(T) | Epoch=018, loss=1.6093, this epoch 0.0108, total 1.5711\n",
      "(T) | Epoch=019, loss=1.6093, this epoch 0.0143, total 1.5854\n",
      "(T) | Epoch=020, loss=1.6086, this epoch 0.0114, total 1.5968\n",
      "(T) | Epoch=021, loss=1.6089, this epoch 0.0108, total 1.6076\n",
      "(T) | Epoch=022, loss=1.6089, this epoch 0.0108, total 1.6184\n",
      "(T) | Epoch=023, loss=1.6093, this epoch 0.0108, total 1.6292\n",
      "(T) | Epoch=024, loss=1.6086, this epoch 0.0108, total 1.6400\n",
      "(T) | Epoch=025, loss=1.6086, this epoch 0.0106, total 1.6506\n",
      "(T) | Epoch=026, loss=1.6093, this epoch 0.0106, total 1.6612\n",
      "(T) | Epoch=027, loss=1.6088, this epoch 0.0103, total 1.6715\n",
      "(T) | Epoch=028, loss=1.6153, this epoch 0.0101, total 1.6816\n",
      "(T) | Epoch=029, loss=1.6091, this epoch 0.0106, total 1.6922\n",
      "(T) | Epoch=030, loss=1.6092, this epoch 0.0103, total 1.7025\n",
      "(T) | Epoch=031, loss=1.6093, this epoch 0.0104, total 1.7128\n",
      "(T) | Epoch=032, loss=1.6092, this epoch 0.0102, total 1.7231\n",
      "(T) | Epoch=033, loss=1.6085, this epoch 0.0104, total 1.7334\n",
      "(T) | Epoch=034, loss=1.6111, this epoch 0.0103, total 1.7438\n",
      "(T) | Epoch=035, loss=1.6093, this epoch 0.0102, total 1.7540\n",
      "(T) | Epoch=036, loss=1.6088, this epoch 0.0105, total 1.7645\n",
      "(T) | Epoch=037, loss=1.6092, this epoch 0.0111, total 1.7756\n",
      "(T) | Epoch=038, loss=1.6109, this epoch 0.0118, total 1.7874\n",
      "(T) | Epoch=039, loss=1.6117, this epoch 0.0110, total 1.7984\n",
      "(T) | Epoch=040, loss=1.6097, this epoch 0.0109, total 1.8093\n",
      "(T) | Epoch=041, loss=1.6092, this epoch 0.0110, total 1.8203\n",
      "(T) | Epoch=042, loss=1.6092, this epoch 0.0109, total 1.8312\n",
      "(T) | Epoch=043, loss=1.6093, this epoch 0.0116, total 1.8428\n",
      "(T) | Epoch=044, loss=1.6092, this epoch 0.0112, total 1.8540\n",
      "(T) | Epoch=045, loss=1.6084, this epoch 0.0112, total 1.8651\n",
      "(T) | Epoch=046, loss=1.6092, this epoch 0.0109, total 1.8761\n",
      "(T) | Epoch=047, loss=1.6092, this epoch 0.0108, total 1.8869\n",
      "(T) | Epoch=048, loss=1.6087, this epoch 0.0104, total 1.8972\n",
      "(T) | Epoch=049, loss=1.6097, this epoch 0.0107, total 1.9079\n",
      "(T) | Epoch=050, loss=1.6081, this epoch 0.0104, total 1.9183\n",
      "(T) | Epoch=051, loss=1.6092, this epoch 0.0104, total 1.9288\n",
      "(T) | Epoch=052, loss=1.6092, this epoch 0.0104, total 1.9392\n",
      "(T) | Epoch=053, loss=1.6100, this epoch 0.0103, total 1.9495\n",
      "(T) | Epoch=054, loss=1.6092, this epoch 0.0105, total 1.9600\n",
      "(T) | Epoch=055, loss=1.6092, this epoch 0.0113, total 1.9713\n",
      "(T) | Epoch=056, loss=1.6092, this epoch 0.0110, total 1.9823\n",
      "(T) | Epoch=057, loss=1.6080, this epoch 0.0121, total 1.9944\n",
      "(T) | Epoch=058, loss=1.6085, this epoch 0.0110, total 2.0055\n",
      "(T) | Epoch=059, loss=1.6091, this epoch 0.0108, total 2.0163\n",
      "(T) | Epoch=060, loss=1.6085, this epoch 0.0108, total 2.0271\n",
      "(T) | Epoch=061, loss=1.6091, this epoch 0.0107, total 2.0378\n",
      "(T) | Epoch=062, loss=1.6081, this epoch 0.0105, total 2.0483\n",
      "(T) | Epoch=063, loss=1.6144, this epoch 0.0104, total 2.0587\n",
      "(T) | Epoch=064, loss=1.6091, this epoch 0.0103, total 2.0690\n",
      "(T) | Epoch=065, loss=1.6080, this epoch 0.0103, total 2.0793\n",
      "(T) | Epoch=066, loss=1.6091, this epoch 0.0107, total 2.0900\n",
      "(T) | Epoch=067, loss=1.6109, this epoch 0.0111, total 2.1011\n",
      "(T) | Epoch=068, loss=1.6080, this epoch 0.0112, total 2.1123\n",
      "(T) | Epoch=069, loss=1.6090, this epoch 0.0111, total 2.1234\n",
      "(T) | Epoch=070, loss=1.6090, this epoch 0.0112, total 2.1345\n",
      "(T) | Epoch=071, loss=1.6082, this epoch 0.0112, total 2.1457\n",
      "(T) | Epoch=072, loss=1.6079, this epoch 0.0108, total 2.1566\n",
      "(T) | Epoch=073, loss=1.6090, this epoch 0.0109, total 2.1675\n",
      "(T) | Epoch=074, loss=1.6090, this epoch 0.0109, total 2.1784\n",
      "(T) | Epoch=075, loss=1.6078, this epoch 0.0106, total 2.1890\n",
      "(T) | Epoch=076, loss=1.6098, this epoch 0.0114, total 2.2004\n",
      "(T) | Epoch=077, loss=1.6093, this epoch 0.0108, total 2.2113\n",
      "(T) | Epoch=078, loss=1.6089, this epoch 0.0104, total 2.2217\n",
      "(T) | Epoch=079, loss=1.6076, this epoch 0.0103, total 2.2319\n",
      "(T) | Epoch=080, loss=1.6075, this epoch 0.0103, total 2.2422\n",
      "(T) | Epoch=081, loss=1.6086, this epoch 0.0106, total 2.2528\n",
      "(T) | Epoch=082, loss=1.6070, this epoch 0.0110, total 2.2638\n",
      "(T) | Epoch=083, loss=1.6106, this epoch 0.0109, total 2.2747\n",
      "(T) | Epoch=084, loss=1.6072, this epoch 0.0108, total 2.2855\n",
      "(T) | Epoch=085, loss=1.6088, this epoch 0.0110, total 2.2965\n",
      "(T) | Epoch=086, loss=1.6076, this epoch 0.0109, total 2.3074\n",
      "(T) | Epoch=087, loss=1.6086, this epoch 0.0109, total 2.3184\n",
      "(T) | Epoch=088, loss=1.6087, this epoch 0.0112, total 2.3296\n",
      "(T) | Epoch=089, loss=1.6074, this epoch 0.0106, total 2.3402\n",
      "(T) | Epoch=090, loss=1.6100, this epoch 0.0105, total 2.3507\n",
      "(T) | Epoch=091, loss=1.6066, this epoch 0.0104, total 2.3611\n",
      "(T) | Epoch=092, loss=1.6095, this epoch 0.0104, total 2.3715\n",
      "(T) | Epoch=093, loss=1.6059, this epoch 0.0105, total 2.3820\n",
      "(T) | Epoch=094, loss=1.6077, this epoch 0.0103, total 2.3923\n",
      "(T) | Epoch=095, loss=1.6069, this epoch 0.0112, total 2.4035\n",
      "(T) | Epoch=096, loss=1.5888, this epoch 0.0106, total 2.4141\n",
      "(T) | Epoch=097, loss=1.6175, this epoch 0.0101, total 2.4242\n",
      "(T) | Epoch=098, loss=1.6075, this epoch 0.0107, total 2.4350\n",
      "(T) | Epoch=099, loss=1.6079, this epoch 0.0108, total 2.4458\n",
      "(T) | Epoch=100, loss=1.6064, this epoch 0.0107, total 2.4564\n",
      "(T) | Epoch=101, loss=1.6053, this epoch 0.0107, total 2.4672\n",
      "(T) | Epoch=102, loss=1.6115, this epoch 0.0108, total 2.4780\n",
      "(T) | Epoch=103, loss=1.6080, this epoch 0.0108, total 2.4887\n",
      "(T) | Epoch=104, loss=1.6076, this epoch 0.0110, total 2.4997\n",
      "(T) | Epoch=105, loss=1.6075, this epoch 0.0111, total 2.5108\n",
      "(T) | Epoch=106, loss=1.6046, this epoch 0.0109, total 2.5217\n",
      "(T) | Epoch=107, loss=1.6073, this epoch 0.0112, total 2.5329\n",
      "(T) | Epoch=108, loss=1.6075, this epoch 0.0112, total 2.5441\n",
      "(T) | Epoch=109, loss=1.6065, this epoch 0.0109, total 2.5550\n",
      "(T) | Epoch=110, loss=1.6075, this epoch 0.0108, total 2.5659\n",
      "(T) | Epoch=111, loss=1.6075, this epoch 0.0110, total 2.5769\n",
      "(T) | Epoch=112, loss=1.6067, this epoch 0.0105, total 2.5873\n",
      "(T) | Epoch=113, loss=1.6045, this epoch 0.0106, total 2.5980\n",
      "(T) | Epoch=114, loss=1.5830, this epoch 0.0112, total 2.6091\n",
      "(T) | Epoch=115, loss=1.6022, this epoch 0.0106, total 2.6197\n",
      "(T) | Epoch=116, loss=1.6060, this epoch 0.0101, total 2.6298\n",
      "(T) | Epoch=117, loss=1.6058, this epoch 0.0101, total 2.6399\n",
      "(T) | Epoch=118, loss=1.6010, this epoch 0.0102, total 2.6500\n",
      "(T) | Epoch=119, loss=1.6095, this epoch 0.0104, total 2.6604\n",
      "(T) | Epoch=120, loss=1.6026, this epoch 0.0101, total 2.6705\n",
      "(T) | Epoch=121, loss=1.6047, this epoch 0.0102, total 2.6808\n",
      "(T) | Epoch=122, loss=1.6045, this epoch 0.0102, total 2.6910\n",
      "(T) | Epoch=123, loss=1.6057, this epoch 0.0107, total 2.7016\n",
      "(T) | Epoch=124, loss=1.5982, this epoch 0.0109, total 2.7126\n",
      "(T) | Epoch=125, loss=1.5991, this epoch 0.0111, total 2.7236\n",
      "(T) | Epoch=126, loss=1.5980, this epoch 0.0109, total 2.7345\n",
      "(T) | Epoch=127, loss=1.5963, this epoch 0.0110, total 2.7456\n",
      "(T) | Epoch=128, loss=1.5947, this epoch 0.0108, total 2.7564\n",
      "(T) | Epoch=129, loss=1.5947, this epoch 0.0110, total 2.7674\n",
      "(T) | Epoch=130, loss=1.6004, this epoch 0.0113, total 2.7787\n",
      "(T) | Epoch=131, loss=1.5915, this epoch 0.0113, total 2.7900\n",
      "(T) | Epoch=132, loss=1.6127, this epoch 0.0109, total 2.8009\n",
      "(T) | Epoch=133, loss=1.6020, this epoch 0.0119, total 2.8128\n",
      "(T) | Epoch=134, loss=1.5890, this epoch 0.0113, total 2.8241\n",
      "(T) | Epoch=135, loss=1.5908, this epoch 0.0108, total 2.8350\n",
      "(T) | Epoch=136, loss=1.6003, this epoch 0.0108, total 2.8458\n",
      "(T) | Epoch=137, loss=1.5943, this epoch 0.0109, total 2.8567\n",
      "(T) | Epoch=138, loss=1.5915, this epoch 0.0103, total 2.8670\n",
      "(T) | Epoch=139, loss=1.5919, this epoch 0.0103, total 2.8774\n",
      "(T) | Epoch=140, loss=1.5888, this epoch 0.0101, total 2.8875\n",
      "(T) | Epoch=141, loss=1.5815, this epoch 0.0108, total 2.8983\n",
      "(T) | Epoch=142, loss=1.5954, this epoch 0.0112, total 2.9096\n",
      "(T) | Epoch=143, loss=1.5855, this epoch 0.0108, total 2.9204\n",
      "(T) | Epoch=144, loss=1.5931, this epoch 0.0111, total 2.9315\n",
      "(T) | Epoch=145, loss=1.5918, this epoch 0.0112, total 2.9428\n",
      "(T) | Epoch=146, loss=1.5730, this epoch 0.0112, total 2.9540\n",
      "(T) | Epoch=147, loss=1.5755, this epoch 0.0116, total 2.9656\n",
      "(T) | Epoch=148, loss=1.5871, this epoch 0.0111, total 2.9767\n",
      "(T) | Epoch=149, loss=1.5702, this epoch 0.0107, total 2.9874\n",
      "(T) | Epoch=150, loss=1.5599, this epoch 0.0108, total 2.9982\n",
      "(T) | Epoch=151, loss=1.4512, this epoch 0.0109, total 3.0090\n",
      "(T) | Epoch=152, loss=1.5613, this epoch 0.0116, total 3.0206\n",
      "(T) | Epoch=153, loss=1.5542, this epoch 0.0109, total 3.0315\n",
      "(T) | Epoch=154, loss=1.5553, this epoch 0.0104, total 3.0419\n",
      "(T) | Epoch=155, loss=1.5486, this epoch 0.0105, total 3.0524\n",
      "(T) | Epoch=156, loss=1.5475, this epoch 0.0104, total 3.0629\n",
      "(T) | Epoch=157, loss=1.5626, this epoch 0.0110, total 3.0739\n",
      "(T) | Epoch=158, loss=1.5370, this epoch 0.0108, total 3.0847\n",
      "(T) | Epoch=159, loss=1.5305, this epoch 0.0109, total 3.0956\n",
      "(T) | Epoch=160, loss=1.5203, this epoch 0.0111, total 3.1067\n",
      "(T) | Epoch=161, loss=1.5420, this epoch 0.0110, total 3.1177\n",
      "(T) | Epoch=162, loss=1.5060, this epoch 0.0110, total 3.1287\n",
      "(T) | Epoch=163, loss=1.5703, this epoch 0.0111, total 3.1398\n",
      "(T) | Epoch=164, loss=1.4897, this epoch 0.0109, total 3.1507\n",
      "(T) | Epoch=165, loss=1.4795, this epoch 0.0103, total 3.1610\n",
      "(T) | Epoch=166, loss=1.4716, this epoch 0.0105, total 3.1715\n",
      "(T) | Epoch=167, loss=1.4952, this epoch 0.0104, total 3.1819\n",
      "(T) | Epoch=168, loss=1.4846, this epoch 0.0103, total 3.1922\n",
      "(T) | Epoch=169, loss=1.4727, this epoch 0.0104, total 3.2026\n",
      "(T) | Epoch=170, loss=1.4737, this epoch 0.0108, total 3.2134\n",
      "(T) | Epoch=171, loss=1.2800, this epoch 0.0119, total 3.2253\n",
      "(T) | Epoch=172, loss=1.4324, this epoch 0.0111, total 3.2364\n",
      "(T) | Epoch=173, loss=1.4168, this epoch 0.0111, total 3.2474\n",
      "(T) | Epoch=174, loss=1.3999, this epoch 0.0109, total 3.2583\n",
      "(T) | Epoch=175, loss=1.3817, this epoch 0.0111, total 3.2694\n",
      "(T) | Epoch=176, loss=1.3235, this epoch 0.0109, total 3.2802\n",
      "(T) | Epoch=177, loss=1.3418, this epoch 0.0110, total 3.2912\n",
      "(T) | Epoch=178, loss=1.2949, this epoch 0.0107, total 3.3020\n",
      "(T) | Epoch=179, loss=1.2975, this epoch 0.0108, total 3.3128\n",
      "(T) | Epoch=180, loss=1.2737, this epoch 0.0110, total 3.3238\n",
      "(T) | Epoch=181, loss=1.4729, this epoch 0.0108, total 3.3345\n",
      "(T) | Epoch=182, loss=1.2204, this epoch 0.0106, total 3.3451\n",
      "(T) | Epoch=183, loss=1.1712, this epoch 0.0105, total 3.3556\n",
      "(T) | Epoch=184, loss=1.1600, this epoch 0.0104, total 3.3660\n",
      "(T) | Epoch=185, loss=1.2545, this epoch 0.0103, total 3.3763\n",
      "(T) | Epoch=186, loss=1.1574, this epoch 0.0103, total 3.3866\n",
      "(T) | Epoch=187, loss=1.2291, this epoch 0.0103, total 3.3969\n",
      "(T) | Epoch=188, loss=1.1199, this epoch 0.0103, total 3.4073\n",
      "(T) | Epoch=189, loss=1.0977, this epoch 0.0103, total 3.4176\n",
      "(T) | Epoch=190, loss=1.0727, this epoch 0.0115, total 3.4291\n",
      "(T) | Epoch=191, loss=1.0238, this epoch 0.0111, total 3.4402\n",
      "(T) | Epoch=192, loss=1.0215, this epoch 0.0106, total 3.4509\n",
      "(T) | Epoch=193, loss=0.9793, this epoch 0.0106, total 3.4615\n",
      "(T) | Epoch=194, loss=0.9606, this epoch 0.0109, total 3.4724\n",
      "(T) | Epoch=195, loss=0.8570, this epoch 0.0108, total 3.4832\n",
      "(T) | Epoch=196, loss=1.0620, this epoch 0.0108, total 3.4940\n",
      "(T) | Epoch=197, loss=0.9341, this epoch 0.0109, total 3.5049\n",
      "(T) | Epoch=198, loss=0.9128, this epoch 0.0109, total 3.5157\n",
      "(T) | Epoch=199, loss=0.9139, this epoch 0.0109, total 3.5266\n",
      "(T) | Epoch=200, loss=0.9039, this epoch 0.0112, total 3.5378\n",
      "=== Final ===\n",
      "tensor([[0.5058, 0.0000, 0.0000],\n",
      "        [0.0834, 0.0000, 0.0000],\n",
      "        [0.5088, 0.0000, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# %load train.py\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter as t\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.datasets import Planetoid, CitationFull\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from model import Encoder, Model, drop_feature\n",
    "from eval import label_classification\n",
    "\n",
    "from data import MyOwnDataset\n",
    "\n",
    "def train(model: Model, x, edge_index):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    edge_index_1 = dropout_adj(edge_index, p=drop_edge_rate_1)[0]\n",
    "    edge_index_2 = dropout_adj(edge_index, p=drop_edge_rate_2)[0]\n",
    "    x_1 = drop_feature(x, drop_feature_rate_1)\n",
    "    x_2 = drop_feature(x, drop_feature_rate_2)\n",
    "    z1 = model(x_1, edge_index_1)\n",
    "    z2 = model(x_2, edge_index_2)\n",
    "\n",
    "    loss = model.loss(z1, z2, batch_size=0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test(model: Model, x, edge_index, final=False):\n",
    "    model.eval()\n",
    "    z = model(x, edge_index)\n",
    "\n",
    "    # label_classification(z, y, ratio=0.1)\n",
    "    print(z)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    data_set_name = 'TEMP'\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', type=str, default=data_set_name)\n",
    "    parser.add_argument('--gpu_id', type=int, default=0)\n",
    "    parser.add_argument('--config', type=str, default='/user_data/huaj/Paper_code/CL/grace1/config.yaml')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    assert args.gpu_id in range(0, 8)\n",
    "    torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "    config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "\n",
    "    torch.manual_seed(config['seed'])\n",
    "    random.seed(12345)\n",
    "\n",
    "    learning_rate = config['learning_rate']\n",
    "    num_hidden = config['num_hidden']\n",
    "    num_proj_hidden = config['num_proj_hidden']\n",
    "    activation = ({'relu': F.relu, 'prelu': nn.PReLU()})[config['activation']]\n",
    "    base_model = ({'GCNConv': GCNConv})[config['base_model']]\n",
    "    num_layers = config['num_layers']\n",
    "\n",
    "    drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "    drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "    drop_feature_rate_1 = config['drop_feature_rate_1']\n",
    "    drop_feature_rate_2 = config['drop_feature_rate_2']\n",
    "    tau = config['tau']\n",
    "    num_epochs = config['num_epochs']\n",
    "    weight_decay = config['weight_decay']\n",
    "\n",
    "    def get_dataset(path, name):\n",
    "        assert name in ['Cora', 'CiteSeer', 'PubMed', data_set_name]\n",
    "        name = data_set_name.lower() if name == data_set_name else name\n",
    "\n",
    "        return MyOwnDataset(\n",
    "            path,\n",
    "            name,\n",
    "            T.NormalizeFeatures())\n",
    "        # return (CitationFull if name == 'dblp' else Planetoid)(\n",
    "        #     path,\n",
    "        #     name,\n",
    "        #     T.NormalizeFeatures())\n",
    "\n",
    "    path = osp.join(osp.expanduser('~'), 'datasets', args.dataset)\n",
    "    dataset = get_dataset(path, args.dataset)\n",
    "    data = dataset[0]\n",
    "    # print(dataset.num_features)\n",
    "    x_t = data['x'].size(dim=1)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "\n",
    "    encoder = Encoder(x_t, num_hidden, activation,\n",
    "                      base_model=base_model, k=num_layers).to(device)\n",
    "    model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    start = t()\n",
    "    prev = start\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        loss = train(model, data.x, data.edge_index)\n",
    "\n",
    "        now = t()\n",
    "        print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "              f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "        prev = now\n",
    "\n",
    "    print(\"=== Final ===\")\n",
    "    test(model, data.x, data.edge_index, final=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bb5ea19dd49b4dde04f6867f0c289efe595c01161296d877c6b15093de6faf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
